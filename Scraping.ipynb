{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRAPING SITO DEL CENTRO RICERCA IN DATA SCIENCE UNIMI\n",
    "\n",
    "* **PROVO A ESTRARRE IL TESTO/CONTENUTO UTILE DELLA HOME** <br>\n",
    "* **CERCO LA LISTA DI PAGINE DEL SITO COLLEGATE ALLA HOME**\n",
    "* **SCARICO IL CONTENUTO ULITE; CIOè SOLO IL TESTO DI TUTTE LE PAGINE**\n",
    "* **COSE DI TEXT MINING PER PRENDERE POROLE PIù FREQ O SU TUTTO IL SITO O SU OGNI PAGINA (DEVO DECIDERE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "import re\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<title>Skills &amp; Expertise – Data Science Research Center (DSRC)</title>\n",
      "Skills & Expertise – Data Science Research Center (DSRC)\n",
      "<p class=\"site-title\"><a href=\"https://datascience.unimi.it/\" rel=\"home\">Data Science Research Center (DSRC)</a></p>\n",
      "<a class=\"skip-link screen-reader-text\" href=\"#content\">Skip to content</a>\n",
      "#content\n"
     ]
    }
   ],
   "source": [
    "url = \"https://datascience.unimi.it/?page_id=136\"\n",
    "response=requests.get(url)\n",
    "print(response)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "#print(soup)\n",
    "\n",
    "print(soup.title)\n",
    "print(soup.title.string)\n",
    "print(soup.p)\n",
    "print(soup.a)\n",
    "print(soup.a[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#content=soup.find('div',{\"class\":\"site-content\"})\n",
    "#content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skills & Expertise \n",
      "\n",
      "COMPUTER SCIENCEdatabase, semantic web, information retrieval, Text mining, security, privacy, security certification, SOA, Computational Intelligence, Image analysis, Cloud Security, Data integration, Knowledge discovery, cyber security, Networks, Crowdsourcing\n",
      "MATH & STATSBootstrap methods, Imputation methods, Hierarchical methods, Customer satisfaction, Meta-analysis, Stochastic Geometry, Topological data analysis, Robust Statistics, Philosophy of Statistics\n",
      "AI & DATA ANALYTICSMachine Learning, Data Stream Analysis, A/B Testing, Personalized Recommendations, Graph Analytics, Intelligent systems, Big data analytics\n",
      "LIFE SCiENCESBiomedicine, Biostatistics, Bioinformatics, Computational biology, Diabetes, Drug discovery, Molecular modeling, Pediatrics, Pharmacology\n",
      "SOCIAL SCIENCESPublic Opinion, Sociology, Political behavior, Political communication, reputation and trust, Scientometrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Skills & Expertise \n",
      "\n",
      "COMPUTER SCIENCEdatabase, semantic web, information retrieval, Text mining, security, privacy, security certification, SOA, Computational Intelligence, Image analysis, Cloud Security, Data integration, Knowledge discovery, cyber security, Networks, Crowdsourcing\n",
      "MATH & STATSBootstrap methods, Imputation methods, Hierarchical methods, Customer satisfaction, Meta-analysis, Stochastic Geometry, Topological data analysis, Robust Statistics, Philosophy of Statistics\n",
      "AI & DATA ANALYTICSMachine Learning, Data Stream Analysis, A/B Testing, Personalized Recommendations, Graph Analytics, Intelligent systems, Big data analytics\n",
      "LIFE SCiENCESBiomedicine, Biostatistics, Bioinformatics, Computational biology, Diabetes, Drug discovery, Molecular modeling, Pediatrics, Pharmacology\n",
      "SOCIAL SCIENCESPublic Opinion, Sociology, Political behavior, Political communication, reputation and trust, Scientometrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "COMPUTER SCIENCEdatabase, semantic web, information retrieval, Text mining, security, privacy, security certification, SOA, Computational Intelligence, Image analysis, Cloud Security, Data integration, Knowledge discovery, cyber security, Networks, Crowdsourcing\n",
      "MATH & STATSBootstrap methods, Imputation methods, Hierarchical methods, Customer satisfaction, Meta-analysis, Stochastic Geometry, Topological data analysis, Robust Statistics, Philosophy of Statistics\n",
      "AI & DATA ANALYTICSMachine Learning, Data Stream Analysis, A/B Testing, Personalized Recommendations, Graph Analytics, Intelligent systems, Big data analytics\n",
      "LIFE SCiENCESBiomedicine, Biostatistics, Bioinformatics, Computational biology, Diabetes, Drug discovery, Molecular modeling, Pediatrics, Pharmacology\n",
      "SOCIAL SCIENCESPublic Opinion, Sociology, Political behavior, Political communication, reputation and trust, Scientometrics\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content=soup.find('div',{\"class\":\"site-content\"})\n",
    "article=''\n",
    "for i in content('div'):\n",
    "    article=article + ' ' + i.text\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prendo la lista delle pagine del sito e cerco di estrarre solo il testo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://datascience.unimi.it/',\n",
       " 'https://datascience.unimi.it/',\n",
       " 'https://datascience.unimi.it/',\n",
       " 'https://datascience.unimi.it/?page_id=101',\n",
       " 'https://datascience.unimi.it/?page_id=87',\n",
       " 'https://datascience.unimi.it/?page_id=136',\n",
       " 'https://datascience.unimi.it/?page_id=90']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_url=[]\n",
    "for link in soup.findAll(\"a\"):\n",
    "    lista_url.append(link.get(\"href\"))\n",
    "    \n",
    "lista_url2= lista_url[1:8]\n",
    "lista_url2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mcatt\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:389: UserWarning: \"https://datascience.unimi.it/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-012f1fc25ee6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent_l\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0marticle_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontent_l\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0marticle_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marticle_l\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_l\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "for i in lista_url2:\n",
    "    response_l=requests.get(i)\n",
    "    print(response_l)\n",
    "    soup_l = BeautifulSoup(i, \"html.parser\")\n",
    "    content_l=soup_l.find('div',{\"class\":\"site-content\"})\n",
    "    print(content_l)\n",
    "    article_l=''\n",
    "    for i in content_l('div'):\n",
    "        article_l=article_l + ' ' + i.text\n",
    "    print(article_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPZIONE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    " \n",
    "def estrapola_sorgente(url):\n",
    "    if 'http://' in url:\n",
    "        sorgente = requests.get(url).text\n",
    "        return(sorgente)\n",
    "    else:\n",
    "        return(\"L'url non è valido\")\n",
    "    \n",
    "def estrapola_nofollow(sorgente):\n",
    "    soup = bs4.BeautifulSoup(sorgente)\n",
    "    elenco = soup.find_all('a')\n",
    "    contatore = 0\n",
    "    if elenco:\n",
    "        for a in elenco:\n",
    "            if a.get('rel'):\n",
    "                if 'nofollow' in a.get('rel')[0]:\n",
    "                    contatore += 1\n",
    "                    print(a.get('href'))\n",
    "        if contatore == 0:\n",
    "            print(\"Non ci sono link con il nofollow\")\n",
    "    else:\n",
    "        print(\"Non ci sono link in questa pagina\")\n",
    "#lista_siti = ['http://datascience.unimi.it']\n",
    "\n",
    "lista_siti = lista_url2\n",
    "for sito in lista_siti:\n",
    "    sorgente = estrapola_sorgente(sito)\n",
    "    print('Elenco dei link nofollow di ' + sito)\n",
    "    estrapola_nofollow(sorgente)\n",
    "    print()\n",
    "\n",
    "for sito in lista_siti:\n",
    "    sorgente = estrapola_sorgente(sito)\n",
    "    soup = bs4.BeautifulSoup(sorgente)\n",
    "    content=soup.find('div',{\"class\":\"site-content\"})\n",
    "    print(content)\n",
    "    #article=''\n",
    "    #for i in content('div'):\n",
    "        #article=article + ' ' + i.text\n",
    "    #print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPZIONE 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "r = requests.get(\"http://datascience.unimi.it\") \n",
    "print(r)\n",
    "contenuto = bs(r.text)\n",
    "print(contenuto.title)\n",
    "print(contenuto.title.string)\n",
    "print(contenuto.p)\n",
    "print(contenuto.a)\n",
    "print(contenuto.a[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contenuto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in contenuto.findAll(\"a\"):\n",
    "    print(link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content=contenuto.find('div',{\"class\":\"site-content\"})\n",
    "article=''\n",
    "for i in content('div'):\n",
    "    article=article + ' ' + i.text\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(contenuto.find_all('li'))):\n",
    "    #print (contenuto.find_all('li')[i].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content=contenuto.find('h5',{\"class\":\"section-subtitle\"})\n",
    "article=''\n",
    "for i in content('div'):\n",
    "    article=article + ' ' + i.text\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPZIONE 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
